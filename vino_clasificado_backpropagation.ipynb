{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "GeH-LRoS6mHM",
        "outputId": "13cb7393-cf72-4726-f2a2-531410eb4e97"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-60e7754f-af46-4e02-9fbb-5edb15e63a4b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-60e7754f-af46-4e02-9fbb-5edb15e63a4b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "TypeError",
          "evalue": "'NoneType' object is not subscriptable",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-3283157670.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 1. Cargar el dataset desde archivo CSV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# ===========================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Abre una ventana para subir archivos y guarda el resultado\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muploaded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Obtiene el nombre del primer archivo subido\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m(target_dir)\u001b[0m\n\u001b[1;32m     70\u001b[0m   \"\"\"\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    169\u001b[0m   \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_collections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m   \u001b[0;32mwhile\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'action'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'complete'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m     result = _output.eval_js(\n\u001b[1;32m    173\u001b[0m         'google.colab._files._uploadFilesContinue(\"{output_id}\")'.format(\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
          ]
        }
      ],
      "source": [
        "\n",
        "import numpy as np # Importa la librería NumPy para operaciones numéricas\n",
        "from google.colab import files # Importa la función files de google.colab para cargar archivos\n",
        "\n",
        "# ===========================\n",
        "# 1. Cargar el dataset desde archivo CSV\n",
        "# ===========================\n",
        "uploaded = files.upload() # Abre una ventana para subir archivos y guarda el resultado\n",
        "filename = list(uploaded.keys())[0] # Obtiene el nombre del primer archivo subido\n",
        "\n",
        "# Leer líneas del archivo\n",
        "with open(filename, 'r') as f: # Abre el archivo subido en modo lectura\n",
        "    lines = f.readlines() # Lee todas las líneas del archivo en una lista\n",
        "\n",
        "# Leer encabezado y determinar qué columnas excluir\n",
        "header = lines[0].strip().split(';') # Lee la primera línea (encabezado), elimina espacios extra y divide por ';'\n",
        "col_excluir = [\"free sulfur dioxide\", \"density\"] # Define una lista de nombres de columnas a excluir\n",
        "\n",
        "# Índices de columnas a excluir\n",
        "# Crea una lista de índices de las columnas cuyo nombre (ignorando mayúsculas y espacios)\n",
        "# está en la lista col_excluir\n",
        "idx_excluir = [i for i, col in enumerate(header) if col.lower().strip() in [c.lower() for c in col_excluir]]\n",
        "\n",
        "# Índices a mantener (todas excepto las excluidas y la última)\n",
        "# Crea una lista de índices para las columnas a incluir, excluyendo las de idx_excluir\n",
        "# y la última columna (que es el target)\n",
        "idx_incluir = [i for i in range(len(header)) if i not in idx_excluir and i != len(header)-1]\n",
        "idx_target = len(header) - 1  # El índice de la última columna (la variable objetivo 'quality')\n",
        "\n",
        "# Procesar filas válidas\n",
        "data_limpia = [] # Inicializa una lista vacía para almacenar los datos limpios\n",
        "for line in lines[1:]: # Itera sobre cada línea del archivo, empezando desde la segunda línea (después del encabezado)\n",
        "    fila = line.strip().split(';') # Elimina espacios extra de la línea y la divide por ';'\n",
        "\n",
        "    # Verifica si la fila tiene el número correcto de columnas y si ninguna celda está vacía\n",
        "    if len(fila) == len(header) and all(cell.strip() != '' for cell in fila):\n",
        "        try:\n",
        "            # Intenta convertir todos los valores de la fila a float\n",
        "            fila_float = [float(cell) for cell in fila]\n",
        "            data_limpia.append(fila_float) # Si la conversión es exitosa, añade la fila a data_limpia\n",
        "        except ValueError:\n",
        "            continue # Si hay un error de conversión (no es un número), ignora la fila\n",
        "\n",
        "# Convertir a NumPy\n",
        "data = np.array(data_limpia, dtype=np.float32) # Convierte la lista de listas data_limpia a un array de NumPy con tipo float32\n",
        "\n",
        "# ===========================\n",
        "# 2. Separar características (X) y variable objetivo (y)\n",
        "# ===========================\n",
        "X = data[:, idx_incluir] # Selecciona todas las filas y las columnas especificadas en idx_incluir para las características (X)\n",
        "y = data[:, idx_target].astype(int) # Selecciona todas las filas y la columna del target, y la convierte a tipo entero\n",
        "\n",
        "print(\"Shape de X:\", X.shape) # Imprime la forma (dimensiones) del array X\n",
        "print(\"Columnas utilizadas (sin las excluidas):\", [header[i] for i in idx_incluir]) # Imprime los nombres de las columnas usadas como características\n",
        "print(\"Primeras 5 muestras de X:\\n\", X[:5]) # Imprime las primeras 5 filas del array X\n",
        "print(\"Valores únicos en y:\\n\", np.unique(y)) # Imprime los valores únicos presentes en la variable objetivo y\n",
        "\n",
        "# ===========================\n",
        "# 3. Normalización (Z-score)\n",
        "# ===========================\n",
        "mean = np.mean(X, axis=0) # Calcula la media de cada columna en X\n",
        "std = np.std(X, axis=0) # Calcula la desviación estándar de cada columna en X\n",
        "X_normalized = (X - mean) / std # Aplica la normalización Z-score: (valor - media) / desviación estándar\n",
        "\n",
        "# ===========================\n",
        "# 4. Codificación One-Hot para salida\n",
        "# ===========================\n",
        "clases_unicas = np.sort(np.unique(y)) # Obtiene los valores únicos en y y los ordena\n",
        "num_clases = len(clases_unicas) # Cuenta el número de clases únicas\n",
        "# Crea un diccionario que mapea cada valor de clase único a un índice (0, 1, 2, ...)\n",
        "class_to_index = {label: idx for idx, label in enumerate(clases_unicas)}\n",
        "y_index = np.array([class_to_index[val] for val in y]) # Convierte cada valor de y a su índice correspondiente usando el diccionario\n",
        "y_onehot = np.zeros((len(y_index), num_clases)) # Crea una matriz de ceros con tantas filas como muestras y tantas columnas como clases\n",
        "y_onehot[np.arange(len(y_index)), y_index] = 1 # Realiza la codificación One-Hot: pone un 1 en la posición del índice de la clase para cada muestra\n",
        "\n",
        "print(\"Ejemplo de salida (y) original:\\n\", y[:5]) # Imprime las primeras 5 muestras de la variable objetivo original\n",
        "print(\"One-hot encoded:\\n\", y_onehot[:5]) # Imprime las primeras 5 muestras de la variable objetivo después de la codificación One-Hot\n",
        "\n",
        "# ===========================\n",
        "# 5. Mezclar y dividir en entrenamiento y prueba\n",
        "# ===========================\n",
        "np.random.seed(42)  # Establece una semilla para el generador de números aleatorios para reproducibilidad\n",
        "indices = np.arange(len(X_normalized)) # Crea un array de índices desde 0 hasta el número de muestras - 1\n",
        "np.random.shuffle(indices)  # Mezcla aleatoriamente los índices\n",
        "X_shuffled = X_normalized[indices] # Reordena las filas de X_normalized según los índices mezclados\n",
        "y_shuffled = y_onehot[indices] # Reordena las filas de y_onehot según los mismos índices mezclados\n",
        "\n",
        "# 60% entrenamiento\n",
        "train_size = int(0.6 * len(X_shuffled)) # Calcula el tamaño del conjunto de entrenamiento (60%)\n",
        "X_train = X_shuffled[:train_size] # Selecciona las primeras 'train_size' filas para el conjunto de entrenamiento X\n",
        "y_train = y_shuffled[:train_size] # Selecciona las primeras 'train_size' filas para el conjunto de entrenamiento y\n",
        "X_test = X_shuffled[train_size:] # Selecciona las filas restantes para el conjunto de prueba X\n",
        "y_test = y_shuffled[train_size:] # Selecciona las filas restantes para el conjunto de prueba y\n",
        "\n",
        "# ===========================\n",
        "# Resumen final\n",
        "# ===========================\n",
        "print(\"Tamaño entrenamiento:\", X_train.shape, y_train.shape) # Imprime las dimensiones de los conjuntos de entrenamiento\n",
        "print(\"Tamaño validación/prueba:\", X_test.shape, y_test.shape) # Imprime las dimensiones de los conjuntos de prueba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oiDdbu-Z1PIo",
        "outputId": "944dad7b-9dc2-46fe-85b8-758c1f923330"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Época 0 | Pérdida: 1.7926 | Precisión: 19.22%\n",
            "Época 100 | Pérdida: 1.4505 | Precisión: 35.54%\n",
            "Época 200 | Pérdida: 1.1668 | Precisión: 52.74%\n",
            "Época 300 | Pérdida: 1.0359 | Precisión: 56.12%\n",
            "Época 400 | Pérdida: 0.9537 | Precisión: 60.81%\n",
            "Época 500 | Pérdida: 0.9070 | Precisión: 62.73%\n",
            "Época 600 | Pérdida: 0.8705 | Precisión: 65.07%\n",
            "Época 700 | Pérdida: 0.8398 | Precisión: 67.60%\n",
            "Época 800 | Pérdida: 0.8126 | Precisión: 69.06%\n",
            "Época 900 | Pérdida: 0.7868 | Precisión: 69.10%\n",
            "Época 1000 | Pérdida: 0.7642 | Precisión: 69.34%\n",
            "Época 1100 | Pérdida: 0.7440 | Precisión: 69.29%\n",
            "Época 1200 | Pérdida: 0.7264 | Precisión: 70.56%\n",
            "Época 1300 | Pérdida: 0.7117 | Precisión: 70.70%\n",
            "Época 1400 | Pérdida: 0.6985 | Precisión: 71.17%\n",
            "Época 1500 | Pérdida: 0.6868 | Precisión: 71.45%\n",
            "Época 1600 | Pérdida: 0.6758 | Precisión: 71.82%\n",
            "Época 1700 | Pérdida: 0.6650 | Precisión: 72.76%\n",
            "Época 1800 | Pérdida: 0.6542 | Precisión: 72.86%\n",
            "Época 1900 | Pérdida: 0.6437 | Precisión: 73.79%\n",
            "Época 2000 | Pérdida: 0.6334 | Precisión: 74.17%\n",
            "Época 2100 | Pérdida: 0.6231 | Precisión: 74.12%\n",
            "Época 2200 | Pérdida: 0.6129 | Precisión: 74.68%\n",
            "Época 2300 | Pérdida: 0.6023 | Precisión: 74.92%\n",
            "Época 2400 | Pérdida: 0.5900 | Precisión: 75.15%\n",
            "Época 2500 | Pérdida: 0.5789 | Precisión: 75.34%\n",
            "Época 2600 | Pérdida: 0.5675 | Precisión: 76.84%\n",
            "Época 2700 | Pérdida: 0.5563 | Precisión: 77.59%\n",
            "Época 2800 | Pérdida: 0.5467 | Precisión: 78.53%\n",
            "Época 2900 | Pérdida: 0.5433 | Precisión: 78.39%\n",
            "Época 3000 | Pérdida: 0.5380 | Precisión: 77.59%\n",
            "Época 3100 | Pérdida: 0.5290 | Precisión: 77.64%\n",
            "Época 3200 | Pérdida: 0.5237 | Precisión: 77.64%\n",
            "Época 3300 | Pérdida: 0.5176 | Precisión: 77.26%\n",
            "Época 3400 | Pérdida: 0.5109 | Precisión: 78.06%\n",
            "Época 3500 | Pérdida: 0.5037 | Precisión: 78.43%\n",
            "Época 3600 | Pérdida: 0.4986 | Precisión: 78.76%\n",
            "Época 3700 | Pérdida: 0.4918 | Precisión: 79.23%\n",
            "Época 3800 | Pérdida: 0.4851 | Precisión: 79.42%\n",
            "Época 3900 | Pérdida: 0.4769 | Precisión: 80.22%\n",
            "Época 4000 | Pérdida: 0.4707 | Precisión: 80.36%\n",
            "Época 4100 | Pérdida: 0.4649 | Precisión: 80.59%\n",
            "Época 4200 | Pérdida: 0.4575 | Precisión: 80.92%\n",
            "Época 4300 | Pérdida: 0.4512 | Precisión: 81.53%\n",
            "Época 4400 | Pérdida: 0.4461 | Precisión: 81.81%\n",
            "Época 4500 | Pérdida: 0.4403 | Precisión: 81.90%\n",
            "Época 4600 | Pérdida: 0.4346 | Precisión: 82.23%\n",
            "Época 4700 | Pérdida: 0.4289 | Precisión: 82.37%\n",
            "Época 4800 | Pérdida: 0.4251 | Precisión: 82.42%\n",
            "Época 4900 | Pérdida: 0.4197 | Precisión: 83.54%\n",
            "Época 5000 | Pérdida: 0.4154 | Precisión: 83.73%\n",
            "Época 5100 | Pérdida: 0.3886 | Precisión: 85.19%\n",
            "Época 5200 | Pérdida: 0.3858 | Precisión: 85.61%\n",
            "Época 5300 | Pérdida: 0.3832 | Precisión: 85.61%\n",
            "Época 5400 | Pérdida: 0.3806 | Precisión: 85.65%\n",
            "Época 5500 | Pérdida: 0.3782 | Precisión: 85.75%\n",
            "Época 5600 | Pérdida: 0.3757 | Precisión: 86.12%\n",
            "Época 5700 | Pérdida: 0.3732 | Precisión: 86.08%\n",
            "Época 5800 | Pérdida: 0.3709 | Precisión: 85.98%\n",
            "Época 5900 | Pérdida: 0.3685 | Precisión: 85.75%\n",
            "Época 6000 | Pérdida: 0.3662 | Precisión: 85.89%\n",
            "Época 6100 | Pérdida: 0.3640 | Precisión: 85.75%\n",
            "Época 6200 | Pérdida: 0.3617 | Precisión: 85.75%\n",
            "Época 6300 | Pérdida: 0.3595 | Precisión: 85.79%\n",
            "Época 6400 | Pérdida: 0.3574 | Precisión: 85.84%\n",
            "Época 6500 | Pérdida: 0.3554 | Precisión: 85.79%\n",
            "Época 6600 | Pérdida: 0.3533 | Precisión: 85.79%\n",
            "Época 6700 | Pérdida: 0.3512 | Precisión: 86.17%\n",
            "Época 6800 | Pérdida: 0.3490 | Precisión: 86.31%\n",
            "Época 6900 | Pérdida: 0.3467 | Precisión: 86.50%\n",
            "Época 7000 | Pérdida: 0.3446 | Precisión: 86.54%\n",
            "Época 7100 | Pérdida: 0.3427 | Precisión: 86.59%\n",
            "Época 7200 | Pérdida: 0.3409 | Precisión: 86.73%\n",
            "Época 7300 | Pérdida: 0.3392 | Precisión: 86.78%\n",
            "Época 7400 | Pérdida: 0.3371 | Precisión: 86.73%\n",
            "Época 7500 | Pérdida: 0.3354 | Precisión: 86.87%\n",
            "Época 7600 | Pérdida: 0.3336 | Precisión: 86.78%\n",
            "Época 7700 | Pérdida: 0.3319 | Precisión: 86.92%\n",
            "Época 7800 | Pérdida: 0.3302 | Precisión: 87.01%\n",
            "Época 7900 | Pérdida: 0.3285 | Precisión: 87.15%\n"
          ]
        }
      ],
      "source": [
        "# === Funciones de activación ===\n",
        "# Implementa la función de activación ReLU (Rectified Linear Unit)\n",
        "def relu(x):\n",
        "    return np.maximum(0, x) # Devuelve el máximo entre 0 y el valor de entrada (si es negativo, se vuelve 0)\n",
        "\n",
        "# Implementa la derivada de la función ReLU, necesaria para la retropropagación\n",
        "def relu_derivative(x):\n",
        "    return (x > 0).astype(float) # Devuelve 1.0 si la entrada es mayor que 0, y 0.0 si es menor o igual a 0\n",
        "\n",
        "# Implementa la función de activación Softmax, utilizada en la capa de salida para problemas de clasificación multiclase\n",
        "def softmax(x):\n",
        "    # Resta el máximo de cada fila para mejorar la estabilidad numérica y evitar la explosión de valores\n",
        "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    # Normaliza los valores exponenciados para que sumen 1 a lo largo de cada fila (probabilidades)\n",
        "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "# === Inicialización de pesos y sesgos ===\n",
        "# Define el tamaño de la capa de entrada (número de características)\n",
        "input_size = X_train.shape[1]         # Ej: 11 si quitaste columnas\n",
        "# Define el tamaño de la primera capa oculta\n",
        "hidden1_size = 16\n",
        "# Define el tamaño de la segunda capa oculta\n",
        "hidden2_size = 32\n",
        "# Define el tamaño de la capa de salida (número de clases)\n",
        "output_size = y_train.shape[1]        # 6 clases (one-hot)\n",
        "\n",
        "# Establece una semilla para el generador de números aleatorios para asegurar la reproducibilidad de la inicialización\n",
        "np.random.seed(1)\n",
        "# Inicializa los pesos de la primera capa con valores aleatorios pequeños para evitar gradientes explosivos/desvanecientes\n",
        "W1 = np.random.randn(input_size, hidden1_size) * 0.1\n",
        "# Inicializa los sesgos de la primera capa con ceros\n",
        "b1 = np.zeros((1, hidden1_size))\n",
        "\n",
        "# Inicializa los pesos de la segunda capa oculta\n",
        "W2 = np.random.randn(hidden1_size, hidden2_size) * 0.1\n",
        "# Inicializa los sesgos de la segunda capa oculta\n",
        "b2 = np.zeros((1, hidden2_size))\n",
        "\n",
        "# Inicializa los pesos de la capa de salida\n",
        "W3 = np.random.randn(hidden2_size, output_size) * 0.1\n",
        "# Inicializa los sesgos de la capa de salida\n",
        "b3 = np.zeros((1, output_size))\n",
        "\n",
        "# === Hiperparámetros ===\n",
        "# Define el número de épocas (iteraciones completas sobre todo el conjunto de entrenamiento)\n",
        "epochs = 8000\n",
        "# Define la tasa de aprendizaje, que controla el tamaño de los pasos durante la actualización de pesos\n",
        "learning_rate = 0.1\n",
        "\n",
        "# === Historial ===\n",
        "# Lista para almacenar el valor de la función de pérdida en cada época\n",
        "loss_history = []\n",
        "# Lista para almacenar la precisión (accuracy) en el conjunto de entrenamiento en cada época\n",
        "accuracy_history = []\n",
        "\n",
        "# === Entrenamiento ===\n",
        "# Bucle principal de entrenamiento que itera a través del número de épocas\n",
        "for epoch in range(epochs):\n",
        "    # --- FORWARD PASS ---\n",
        "    # Calcula la entrada a la primera capa oculta (multiplicación matricial de X_train y W1, más el sesgo b1)\n",
        "    Z1 = X_train @ W1 + b1\n",
        "    # Aplica la función de activación ReLU a la salida de la primera capa oculta\n",
        "    A1 = relu(Z1)\n",
        "\n",
        "    # Calcula la entrada a la segunda capa oculta\n",
        "    Z2 = A1 @ W2 + b2\n",
        "    # Aplica la función de activación ReLU a la salida de la segunda capa oculta\n",
        "    A2 = relu(Z2)\n",
        "\n",
        "    # Calcula la entrada a la capa de salida\n",
        "    Z3 = A2 @ W3 + b3\n",
        "    # Aplica la función de activación Softmax a la salida de la capa de salida para obtener probabilidades\n",
        "    A3 = softmax(Z3)\n",
        "\n",
        "    # --- FUNCIÓN DE PÉRDIDA (Cross-Entropy) ---\n",
        "    # Calcula la pérdida de entropía cruzada, común para problemas de clasificación multiclase\n",
        "    # np.sum(y_train * np.log(A3 + 1e-8), axis=1) calcula la pérdida para cada muestra\n",
        "    # -np.mean(...) calcula el promedio de la pérdida sobre todas las muestras del lote de entrenamiento\n",
        "    loss = -np.mean(np.sum(y_train * np.log(A3 + 1e-8), axis=1)) # Se añade un pequeño valor (1e-8) para evitar logaritmo de cero\n",
        "    # Almacena el valor de la pérdida en el historial\n",
        "    loss_history.append(loss)\n",
        "\n",
        "    # --- ACCURACY ---\n",
        "    # Obtiene las clases predichas tomando el índice del valor máximo en la salida de Softmax (A3)\n",
        "    y_pred_train = np.argmax(A3, axis=1)\n",
        "    # Obtiene las clases verdaderas tomando el índice del valor máximo en la codificación One-Hot (y_train)\n",
        "    y_true_train = np.argmax(y_train, axis=1)\n",
        "    # Calcula la precisión comparando las clases predichas con las verdaderas\n",
        "    acc_train = np.mean(y_pred_train == y_true_train)\n",
        "    # Almacena la precisión en el historial\n",
        "    accuracy_history.append(acc_train)\n",
        "\n",
        "    # --- BACKPROPAGATION ---\n",
        "    # Calcula el gradiente de la pérdida con respecto a la entrada de la capa de salida (Z3)\n",
        "    dZ3 = A3 - y_train\n",
        "    # Calcula el gradiente de la pérdida con respecto a los pesos de la capa de salida (W3)\n",
        "    dW3 = A2.T @ dZ3\n",
        "    # Calcula el gradiente de la pérdida con respecto a los sesgos de la capa de salida (b3)\n",
        "    db3 = np.sum(dZ3, axis=0, keepdims=True)\n",
        "\n",
        "    # Calcula el gradiente de la pérdida con respecto a la salida de la segunda capa oculta (A2)\n",
        "    dA2 = dZ3 @ W3.T\n",
        "    # Calcula el gradiente de la pérdida con respecto a la entrada de la segunda capa oculta (Z2), aplicando la derivada de ReLU\n",
        "    dZ2 = dA2 * relu_derivative(Z2)\n",
        "    # Calcula el gradiente de la pérdida con respecto a los pesos de la segunda capa oculta (W2)\n",
        "    dW2 = A1.T @ dZ2\n",
        "    # Calcula el gradiente de la pérdida con respecto a los sesgos de la segunda capa oculta (b2)\n",
        "    db2 = np.sum(dZ2, axis=0, keepdims=True)\n",
        "\n",
        "    # Calcula el gradiente de la pérdida con respecto a la salida de la primera capa oculta (A1)\n",
        "    dA1 = dZ2 @ W2.T\n",
        "    # Calcula el gradiente de la pérdida con respecto a la entrada de la primera capa oculta (Z1), aplicando la derivada de ReLU\n",
        "    dZ1 = dA1 * relu_derivative(Z1)\n",
        "    # Calcula el gradiente de la pérdida con respecto a los pesos de la primera capa oculta (W1)\n",
        "    dW1 = X_train.T @ dZ1\n",
        "    # Calcula el gradiente de la pérdida con respecto a los sesgos de la primera capa oculta (b1)\n",
        "    db1 = np.sum(dZ1, axis=0, keepdims=True)\n",
        "\n",
        "    # --- ACTUALIZACIÓN DE PESOS ---\n",
        "    # Actualiza los pesos y sesgos de la capa de salida utilizando el descenso de gradiente\n",
        "    # Se divide por el número de muestras (X_train.shape[0]) para obtener el gradiente promedio del lote\n",
        "    W3 -= learning_rate * dW3 / X_train.shape[0]\n",
        "    b3 -= learning_rate * db3 / X_train.shape[0]\n",
        "    # Actualiza los pesos y sesgos de la segunda capa oculta\n",
        "    W2 -= learning_rate * dW2 / X_train.shape[0]\n",
        "    b2 -= learning_rate * db2 / X_train.shape[0]\n",
        "    # Actualiza los pesos y sesgos de la primera capa oculta\n",
        "    W1 -= learning_rate * dW1 / X_train.shape[0]\n",
        "    b1 -= learning_rate * db1 / X_train.shape[0]\n",
        "\n",
        "    # --- Ajuste de Learning Rate ---\n",
        "    if epoch > 5000 and epoch <= 6500:\n",
        "        learning_rate = 0.05\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # --- Mostrar progreso ---\n",
        "    # Imprime el progreso del entrenamiento (época, pérdida y precisión) cada 100 épocas\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Época {epoch} | Pérdida: {loss:.4f} | Precisión: {acc_train * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Ss3V8okBQLs",
        "outputId": "0feb35e9-8cf5-462f-bd8f-665ac939c07a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🔍 Precisión (accuracy) del modelo: 81.22%\n",
            "\n",
            " Matriz de Confusión (filas: real, columnas: predicho):\n",
            "[[277   0   0   0   0   0]\n",
            " [  0 266   3   3   0   0]\n",
            " [  6  17 177  63   8   0]\n",
            " [  0  13  70 129  23  12]\n",
            " [  1   3   6  30  21   9]\n",
            " [  0   0   0   0   0 285]]\n",
            "\n",
            "Ejemplos de predicción:\n",
            "Vino 1: Real = 8, Predicho = 8\n",
            "Vino 2: Real = 3, Predicho = 3\n",
            "Vino 3: Real = 4, Predicho = 4\n",
            "Vino 4: Real = 3, Predicho = 3\n",
            "Vino 5: Real = 8, Predicho = 8\n",
            "Vino 6: Real = 8, Predicho = 8\n",
            "Vino 7: Real = 3, Predicho = 3\n",
            "Vino 8: Real = 8, Predicho = 8\n",
            "Vino 9: Real = 6, Predicho = 6\n",
            "Vino 10: Real = 5, Predicho = 5\n"
          ]
        }
      ],
      "source": [
        "# === EVALUACIÓN ===\n",
        "\n",
        "# 1. Hacer forward pass con los datos de prueba\n",
        "# Calcula la entrada a la primera capa oculta usando los datos de prueba (X_test)\n",
        "Z1_test = X_test @ W1 + b1\n",
        "# Aplica la función de activación ReLU a la salida de la primera capa oculta en el conjunto de prueba\n",
        "A1_test = relu(Z1_test)\n",
        "\n",
        "# Calcula la entrada a la segunda capa oculta usando la salida de la capa anterior (A1_test)\n",
        "Z2_test = A1_test @ W2 + b2\n",
        "# Aplica la función de activación ReLU a la salida de la segunda capa oculta en el conjunto de prueba\n",
        "A2_test = relu(Z2_test)\n",
        "\n",
        "# Calcula la entrada a la capa de salida usando la salida de la capa anterior (A2_test)\n",
        "Z3_test = A2_test @ W3 + b3\n",
        "# Aplica la función de activación Softmax a la salida de la capa de salida para obtener probabilidades de predicción\n",
        "A3_test = softmax(Z3_test)\n",
        "\n",
        "# 2. Obtener clases predichas y verdaderas\n",
        "# Obtiene las clases predichas tomando el índice del valor máximo en la salida de Softmax (A3_test) para cada muestra\n",
        "y_pred = np.argmax(A3_test, axis=1)\n",
        "# Obtiene las clases verdaderas tomando el índice del valor máximo en la codificación One-Hot (y_test) para cada muestra\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "# 3. Calcular accuracy\n",
        "# Calcula la precisión comparando las clases predichas (y_pred) con las clases verdaderas (y_true)\n",
        "accuracy = np.mean(y_pred == y_true)\n",
        "# Imprime la precisión del modelo en el conjunto de prueba, formateada a dos decimales\n",
        "print(f\"\\n🔍 Precisión (accuracy) del modelo: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# 4. Matriz de confusión\n",
        "conf_matrix = np.zeros((6, 6), dtype=int)  # Inicializa una matriz de ceros de tamaño 6x6 (para 6 clases) con tipo entero\n",
        "\n",
        "# Itera sobre cada muestra en el conjunto de prueba\n",
        "for i in range(len(y_true)):\n",
        "    true_class = y_true[i] # Obtiene la clase verdadera de la muestra actual\n",
        "    pred_class = y_pred[i] # Obtiene la clase predicha de la muestra actual\n",
        "    conf_matrix[true_class, pred_class] += 1 # Incrementa el contador en la matriz de confusión en la posición (clase_verdadera, clase_predicha)\n",
        "\n",
        "# Imprime la matriz de confusión con una etiqueta explicativa\n",
        "print(\"\\n Matriz de Confusión (filas: real, columnas: predicho):\")\n",
        "print(conf_matrix) # Imprime la matriz de confusión calculada\n",
        "\n",
        "# 5. Mostrar algunas predicciones\n",
        "print(\"\\nEjemplos de predicción:\")\n",
        "# Itera sobre las primeras 10 muestras del conjunto de prueba\n",
        "for i in range(10):\n",
        "    real = y_true[i] # Obtiene la clase verdadera (índice)\n",
        "    pred = y_pred[i] # Obtiene la clase predicha (índice)\n",
        "    # Imprime la clase verdadera y predicha para cada ejemplo, sumando 3 para mostrar las clases originales (3 a 8)\n",
        "    print(f\"Vino {i+1}: Real = {real + 3}, Predicho = {pred + 3}\")  # sumamos 3 para volver a clases reales"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
